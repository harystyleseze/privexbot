"""
Chunk model - Text segments from documents for RAG retrieval.

WHY:
- RAG requires small, focused pieces of text
- Embeddings work better on chunks than full documents
- Enable precise retrieval and source attribution
- Core unit of semantic search

HOW:
- Documents are split into chunks using various strategies
- Each chunk embedded as vector for similarity search
- Stored in both database and vector store
- Retrieved based on semantic similarity to query

PSEUDOCODE:
-----------
class Chunk(Base):
    __tablename__ = "chunks"

    # Identity
    id: UUID (primary key, auto-generated)
        WHY: Unique identifier for this chunk

    document_id: UUID (foreign key -> documents.id, indexed, cascade delete)
        WHY: Links to parent document
        HOW: When document deleted, all chunks deleted

    # Content
    content: text (required)
        WHY: The actual text content of this chunk
        HOW: Result of chunking strategy applied to document
        SIZE: Typically 200-2000 characters depending on strategy

    content_hash: str (indexed)
        WHY: Detect duplicate chunks
        HOW: SHA256 hash of content
        USE: Avoid re-embedding identical chunks

    # Position and Context
    position: int (indexed)
        WHY: Order of chunks within document
        HOW: 0-indexed, maintains document structure
        USE: Display chunks in original order

    chunk_index: int
        WHY: Global chunk number (for debugging)
        EXAMPLE: "chunk_45_of_230"

    page_number: int | None
        WHY: Track which page chunk came from
        HOW: Extracted during parsing (for PDFs, DOCX)
        USE: Show source page to user

    # Chunk Metadata
    chunk_metadata: JSONB
        WHY: Rich context about chunk origin and structure

        STRUCTURE:
        {
            # Structure Information
            "heading": "Chapter 3: Installation",
                WHY: Section/chapter this chunk belongs to

            "heading_level": 2,
                WHY: H1, H2, H3, etc.

            "parent_section": "Getting Started",
                WHY: Hierarchical context

            # Source Details
            "start_char": 5000,
            "end_char": 6200,
                WHY: Character positions in original document

            "start_page": 12,
            "end_page": 13,
                WHY: Page range (if multi-page chunk)

            # Element Type
            "element_type": "text" | "table" | "list" | "code" | "quote",
                WHY: Different types may need different handling

            "original_element_id": "para_42",
                WHY: Reference to original parsed element

            # Contextual Information (for contextual chunking)
            "prefix": "This section covers installation steps for Linux systems",
                WHY: Brief explanation of chunk context
                HOW: Improves retrieval and LLM understanding

            # Language and Formatting
            "language": "en",
            "contains_code": true,
            "code_language": "python",

            # Quality Signals
            "is_truncated": false,
                WHY: Track if chunk was cut off

            "overlaps_with": [prev_chunk_id, next_chunk_id],
                WHY: Track overlap for context preservation
        }

    # Size Metrics
    word_count: int
    character_count: int
    token_count: int | None
        WHY: Track for LLM context limits
        HOW: Calculated using tiktoken or similar

    # Embedding
    embedding: vector | None
        WHY: Vector representation for semantic search
        HOW: Generated by embedding model
        TYPE: vector(dimensions) - e.g., vector(1536) for OpenAI, vector(768) for others

        NOTE: This field is OPTIONAL in database
        WHY: Large vectors may be stored only in vector store
        HOW: embedding_id references vector in external store

    embedding_id: str | None
        WHY: Reference to embedding in external vector store
        EXAMPLE: "{kb_id}_{chunk_id}" for FAISS, Qdrant, etc.
        USE: Retrieve from vector store when needed

    embedding_metadata: JSONB
        WHY: Track embedding generation details

        STRUCTURE:
        {
            "model": "text-embedding-ada-002",
            "provider": "openai",
            "dimensions": 1536,
            "generated_at": "2024-01-15T10:30:00Z",
            "generation_time_ms": 245,
            "batch_id": "batch_123",
                WHY: Group embeddings generated together
        }

    # Search Optimization
    keywords: list[str] | None
        WHY: Support keyword search in addition to semantic
        HOW: Extracted key terms from chunk
        EXAMPLE: ["installation", "linux", "setup", "dependencies"]

    # Quality and Status
    is_enabled: bool (default: True)
        WHY: Disable chunks without deleting
        HOW: Disabled chunks not included in search results

    is_edited: bool (default: False)
        WHY: Track manual edits
        HOW: Set when user manually modifies chunk content
        USE: Show "edited" badge in UI

    quality_score: float | None (0.0 to 1.0)
        WHY: Rank chunk quality
        HOW: Calculated based on:
            - Length (not too short/long)
            - Coherence
            - Information density
        USE: Filter low-quality chunks

    # Relationships and References
    related_chunks: list[UUID] | None
        WHY: Link semantically similar chunks
        HOW: Found during embedding generation
        USE: Expand search results

    # Timestamps
    created_at: datetime (auto-set)
    updated_at: datetime (auto-update)
    last_retrieved_at: datetime | None
        WHY: Track which chunks are actually used
        HOW: Updated when chunk returned in search

    retrieval_count: int (default: 0)
        WHY: Popularity metric
        USE: Prioritize frequently retrieved chunks

    # Indexes
    index: (document_id, position)
        WHY: Fast ordered retrieval of document chunks

    index: (embedding) using vector index
        WHY: Fast similarity search
        HOW: IVFFlat, HNSW, or other vector index types
        NOTE: Only if storing embeddings in PostgreSQL

    index: (content_hash)
        WHY: Duplicate detection

    index: (is_enabled)
        WHY: Filter active chunks

    index: (keywords) using GIN
        WHY: Fast keyword search

    # Computed Fields
    @property
    def preview(self) -> str:
        WHY: Show snippet in UI
        return self.content[:200] + "..." if len(self.content) > 200 else self.content

    @property
    def source_info(self) -> dict:
        WHY: Quick access to source document info
        return {
            "document_name": self.document.name,
            "page": self.page_number,
            "position": self.position
        }

CHUNKING STRATEGIES RECAP:
---------------------------
1. **Size-Based**: Split at max_characters, preserve word boundaries
2. **By Heading**: Chunk per section/chapter
3. **By Page**: One chunk per page
4. **By Similarity**: Group semantically related content

CHUNK LIFECYCLE:
----------------
1. Document Upload:
    → Parse document → Extract elements

2. Chunking:
    → Apply strategy → Create chunks → Store in DB

3. Embedding:
    → Generate embeddings → Store in vector DB → Link via embedding_id

4. Retrieval:
    → Query comes in → Search vector store → Return matching chunks

5. Usage Tracking:
    → Update last_retrieved_at → Increment retrieval_count

6. Maintenance:
    → Remove low-quality chunks
    → Re-chunk if strategy changes
    → Update embeddings if model changes

VECTOR STORE SYNC:
------------------
WHY: Database and vector store must stay in sync
HOW: Dual-write pattern with verification

CREATE:
    1. Insert chunk in database
    2. Generate embedding
    3. Upsert to vector store with metadata
    4. Store embedding_id in database
    5. Verify both succeeded

UPDATE (content change):
    1. Update chunk content in database
    2. Set is_edited = True
    3. Re-generate embedding
    4. Update vector store
    5. Update embedding_metadata

DELETE:
    1. Delete from vector store (using embedding_id)
    2. Delete from database (cascade)
    3. Verify both succeeded

METADATA IN VECTOR STORE:
--------------------------
WHY: Enable filtering during search
HOW: Most vector DBs support metadata alongside vectors

STORED METADATA:
    {
        "chunk_id": "uuid",
        "document_id": "uuid",
        "document_name": "FAQ.pdf",
        "page_number": 5,
        "heading": "Pricing",
        "custom_metadata": {  # From parent document
            "department": "Sales",
            "version": "2.0"
        },
        "created_at": "2024-01-15",
        "is_enabled": true
    }

SEARCH WITH FILTERS:
    query_embedding = embed("pricing information")
    filters = {
        "page_number": {"$gte": 1, "$lte": 10"},
        "custom_metadata.department": "Sales",
        "is_enabled": True
    }
    results = vector_store.search(query_embedding, filters=filters, top_k=5)

HYBRID SEARCH:
--------------
WHY: Combine semantic and keyword search for better results
HOW: Parallel search + result fusion

PROCESS:
    1. Semantic Search:
        embedding = embed(query)
        semantic_results = vector_store.search(embedding, top_k=20)

    2. Keyword Search:
        keyword_results = db.query(Chunk).filter(
            Chunk.keywords.overlap([query_keywords])
        ).limit(20)

    3. Fusion (Reciprocal Rank Fusion):
        combined = merge_and_rerank(semantic_results, keyword_results)
        return combined[:5]

CONTEXTUAL CHUNKING:
--------------------
WHY: Add context to improve retrieval
HOW: Prepend brief explanation to chunk

EXAMPLE:
    Original chunk: "Run the following command: sudo apt install..."

    With context prefix:
        "This chunk explains how to install dependencies on Linux systems.
         Run the following command: sudo apt install..."

    Embedding includes prefix → Better matches for "how to install on linux"

QUALITY SCORING:
----------------
WHY: Filter out poor chunks
HOW: Heuristic scoring

FACTORS:
    - Length: 0.0 if <50 chars or >3000 chars, 1.0 if 200-1500 chars
    - Coherence: Has complete sentences
    - Information density: Not just repeated words
    - Structure: Has proper formatting

USE:
    # Only use high-quality chunks
    chunks = Chunk.query.filter(
        Chunk.quality_score >= 0.7,
        Chunk.is_enabled == True
    )

API ACCESS:
-----------
WHY: Platform feature - developers can query chunks directly
HOW: Public API endpoint with authentication

EXAMPLE:
    GET /api/v1/documents/{doc_id}/chunks
    → List all chunks

    GET /api/v1/chunks/{chunk_id}
    → Get specific chunk with embedding

    POST /api/v1/chunks/{chunk_id}/feedback
    Body: {"relevant": true, "comment": "Very helpful"}
    → Track chunk relevance for improvement
"""
