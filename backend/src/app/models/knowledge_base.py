"""
KnowledgeBase model - Centralized knowledge storage for chatbots and chatflows.

WHY:
- Single source of truth for RAG (Retrieval-Augmented Generation)
- Shared knowledge across multiple bots (avoid duplication)
- Context-aware access control (which bot can use which knowledge)
- Platform feature: Users build knowledge once, use in multiple bots

HOW:
- Lives within workspace (tenant isolation)
- Documents are chunked and embedded for semantic search
- Context settings control which chatbot/chatflow can access
- Supports multiple vector store backends

KEY DESIGN PRINCIPLE:
- ONE knowledge base can serve MANY chatbots/chatflows
- Access is controlled via context_settings (not separate link tables)
- This enables flexible knowledge sharing within workspace

PSEUDOCODE:
-----------
class KnowledgeBase(Base):
    __tablename__ = "knowledge_bases"

    # Identity
    id: UUID (primary key, auto-generated)
        WHY: Unique identifier for this knowledge base

    name: str (required, max_length=100)
        WHY: Human-readable name
        EXAMPLE: "Product Documentation", "FAQ Database", "Customer Support KB"

    description: text | None
        WHY: Explain what knowledge this contains
        EXAMPLE: "Contains all product manuals and user guides"

    workspace_id: UUID (foreign key -> workspaces.id, indexed, cascade delete)
        WHY: CRITICAL FOR TENANCY - Links to workspace
        HOW: All knowledge bases isolated by workspace
        SECURITY: Cannot access KB from different workspace

    # Access Control (Context-Aware Design)
    context_settings: JSONB
        WHY: Define which bots can use this knowledge and how
        HOW: Flexible per-bot access configuration

        STRUCTURE:
        {
            "access_mode": "all" | "specific" | "none",
                WHY: Control default access
                - "all": All chatbots/chatflows in workspace can use
                - "specific": Only specified bots can use
                - "none": No automatic access (manual linking required)

            "allowed_chatbots": [UUID, UUID, ...],
                WHY: Specific chatbot IDs that can access (if mode="specific")

            "allowed_chatflows": [UUID, UUID, ...],
                WHY: Specific chatflow IDs that can access (if mode="specific")

            "retrieval_config": {
                "top_k": 5,
                    WHY: How many chunks to retrieve per query

                "similarity_threshold": 0.7,
                    WHY: Minimum similarity score to include chunk

                "search_method": "semantic" | "keyword" | "hybrid",
                    WHY: How to search for relevant chunks

                "rerank": true,
                    WHY: Re-rank results for better relevance

                "include_metadata": true,
                    WHY: Return chunk metadata with results
            },

            "usage_limits": {
                "max_queries_per_day": 1000,
                    WHY: Prevent abuse in API usage

                "max_tokens_per_query": 2000,
                    WHY: Control context size
            }
        }

    # Embedding Configuration
    embedding_config: JSONB
        WHY: Store embedding model and settings
        HOW: Allows switching models or updating settings

        STRUCTURE:
        {
            "provider": "openai" | "secret_ai" | "huggingface" | "custom",
            "model": "text-embedding-ada-002" | "multi-qa-mpnet-base-dot-v1",
            "dimensions": 1536,
                WHY: Vector size for this model
            "batch_size": 100,
                WHY: How many chunks to embed at once
        }

    # Vector Store Configuration (FLEXIBLE DESIGN)
    vector_store_config: JSONB
        WHY: Support ANY vector database backend
        HOW: Abstraction layer handles different providers

        STRUCTURE:
        {
            "provider": "faiss" | "weaviate" | "qdrant" | "milvus" | "chroma" | "pinecone" | "redis" | "elasticsearch" | "vespa" | "vald",
                WHY: Which vector database to use

            "connection": {
                # Provider-specific connection details
                # Examples for different providers:

                # FAISS (local, file-based)
                "type": "faiss",
                "index_path": "/data/kb_{kb_id}/faiss.index",
                "index_type": "IndexFlatIP" | "IndexIVFFlat" | "IndexHNSW",

                # Weaviate (cloud/self-hosted)
                "type": "weaviate",
                "url": "https://your-cluster.weaviate.network",
                "api_key": "encrypted_key",
                "class_name": "KnowledgeChunk",

                # Qdrant (cloud/self-hosted)
                "type": "qdrant",
                "url": "http://localhost:6333",
                "api_key": "encrypted_key",
                "collection_name": "kb_{kb_id}",

                # Milvus (cloud/self-hosted)
                "type": "milvus",
                "host": "localhost",
                "port": 19530,
                "collection_name": "knowledge_base_{kb_id}",

                # Pinecone (cloud)
                "type": "pinecone",
                "api_key": "encrypted_key",
                "environment": "us-west1-gcp",
                "index_name": "privexbot-kb-{kb_id}",

                # Redis (with RediSearch)
                "type": "redis",
                "url": "redis://localhost:6379",
                "index_name": "kb:{kb_id}",

                # Chroma (local/cloud)
                "type": "chroma",
                "host": "localhost",
                "port": 8000,
                "collection_name": "kb_{kb_id}",
            },

            "metadata_config": {
                "store_full_content": true,
                    WHY: Whether to store full chunk text in vector DB or just reference

                "indexed_fields": ["document_name", "page_number", "chunk_type"],
                    WHY: Which metadata fields to index for filtering

                "filterable_fields": ["document_id", "created_at", "custom_tags"],
                    WHY: Enable metadata filtering in queries
            },

            "performance": {
                "cache_enabled": true,
                "cache_ttl": 3600,
                "batch_upsert": true,
            }
        }

    # Indexing Settings
    indexing_method: str (default: "high_quality")
        WHY: Control indexing quality vs speed
        OPTIONS: "high_quality", "balanced", "fast"

    reindex_required: bool (default: False)
        WHY: Flag when settings change require re-indexing
        HOW: Background job processes when True

    # Statistics
    total_documents: int (default: 0)
    total_chunks: int (default: 0)
    total_tokens: int (default: 0)
    last_indexed_at: datetime | None

    # Metadata
    created_by: UUID (foreign key -> users.id)
    created_at: datetime (auto-set)
    updated_at: datetime (auto-update)

    # Relationships
    workspace: Workspace (many-to-one)
        WHY: Access parent workspace and org

    documents: list[Document] (one-to-many, cascade delete)
        WHY: All documents in this KB
        HOW: When KB deleted, all documents and chunks deleted

    creator: User (many-to-one)
        WHY: Audit trail

CONTEXT-AWARE ACCESS PATTERN:
------------------------------
WHY: Single KB serves multiple bots with different access needs

EXAMPLE 1: Public KB for all bots
    context_settings = {
        "access_mode": "all",
        "retrieval_config": {"top_k": 5}
    }
    → All chatbots/chatflows in workspace can use this KB

EXAMPLE 2: Specific bot access
    context_settings = {
        "access_mode": "specific",
        "allowed_chatbots": [chatbot1_id, chatbot2_id],
        "allowed_chatflows": [chatflow1_id]
    }
    → Only specified bots can use this KB

EXAMPLE 3: Different retrieval per bot type
    Query from chatbot: top_k=5, semantic search
    Query from chatflow: top_k=10, hybrid search
    → Can be customized in bot's KB reference

USAGE IN CHATBOT/CHATFLOW:
---------------------------
Instead of many-to-many association table, chatbot/chatflow config includes:

Chatbot config:
{
    "knowledge_bases": [
        {
            "kb_id": "uuid",
            "enabled": true,
            "override_retrieval": {
                "top_k": 3,  # Override default
                "search_method": "hybrid"
            }
        }
    ]
}

WHY this design:
- More flexible than association table
- Bot-specific retrieval overrides
- Can disable KB without deleting link
- Easier to manage in UI

TENANT ISOLATION:
-----------------
def get_knowledge_base(kb_id: UUID, current_user):
    kb = db.query(KnowledgeBase)
        .join(Workspace)
        .join(Organization)
        .filter(
            KnowledgeBase.id == kb_id,
            Organization.id == current_user.org_id
        )
        .first()

    if not kb:
        raise HTTPException(404, "Knowledge base not found")
    return kb

VECTOR STORE FLEXIBILITY:
--------------------------
WHY: Different users have different needs
- Startups: FAISS (free, local)
- Scale-ups: Qdrant/Weaviate (self-hosted, cost-effective)
- Enterprise: Pinecone/Milvus (managed, scalable)

HOW: Abstraction layer (vector_store_service.py) handles all providers:

function search_similar_chunks(kb: KnowledgeBase, query_embedding: list[float]):
    provider = kb.vector_store_config["provider"]

    if provider == "faiss":
        return faiss_adapter.search(kb, query_embedding)
    elif provider == "qdrant":
        return qdrant_adapter.search(kb, query_embedding)
    elif provider == "weaviate":
        return weaviate_adapter.search(kb, query_embedding)
    # ... etc for all providers

DEPLOYMENT & API ACCESS:
-------------------------
WHY: Platform feature - users can call KB API from anywhere

Each Knowledge Base can have:
- API endpoint: POST /api/v1/kb/{kb_id}/query
- Requires API key (generated per KB or workspace)
- Public access for deployed chatbots

EXAMPLE API CALL:
    curl -X POST https://api.privexbot.com/v1/kb/{kb_id}/query \
        -H "Authorization: Bearer {api_key}" \
        -d '{
            "query": "How do I reset my password?",
            "top_k": 5,
            "filters": {"document_name": "FAQ"}
        }'

MIGRATION SUPPORT:
------------------
WHY: Users may want to switch vector stores
HOW: Background task to migrate embeddings

function migrate_vector_store(kb_id: UUID, new_provider: str):
    1. Get all chunks with embeddings
    2. Initialize new vector store
    3. Batch upsert to new store
    4. Verify migration
    5. Update kb.vector_store_config
    6. Delete old vector store data
"""
