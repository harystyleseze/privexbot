âœ… 1. Crawl Step (/sources/web) â€” Raw HTML/markdown

âŒ Not what you want.

This step should only downloads the raw HTML and discovers links.
It does not extract clean text.

You will still have:

Menus

Headers

Navigation bars

Footer text

Buttons

Sidebar content

Repeated sections

âŒ 2. Preview Step (/preview) â€” Small Sample Only

âŒ Also not what you want.

Preview = just a few chunks, quickly extracted, no cleanup.
Good for preview cases by users as implemented
Not full content and not cleaned. Thatâ€™s why the content it generated had navigation, emojis, menus.

â­ 3. Process Step (/process) â€” FULL CLEANED CONTENT (we need a clean web page content )

âœ”ï¸ THIS is the step that produces the full, cleaned page content.

This is where the system:

ğŸ§¹ Cleans the HTML

removes nav bars

removes footers

removes menus

removes duplicate content

removes javascript UI elements

ğŸ“„ Extracts true page text

headings

paragraphs

code blocks

documentation text

lists

tables

images 

use OCR too

âœ‚ï¸ Splits into meaningful chunks

Following your config:

{
  "chunk_size": 1000,
  "chunk_overlap": 200
}

ğŸ“‘ Stores chunks page-by-page

This should be the only step that produces the actual â€œusableâ€ knowledge base content.

âœ”ï¸ 4. After processing, you can fetch:
All cleaned pages
GET /api/v1/kb-drafts/{draft_id}/pages

One cleaned page
GET /api/v1/kb-drafts/{draft_id}/pages/{page_index}

All chunks
GET /api/v1/kb-drafts/{draft_id}/chunks

Chunks for specific page
GET /api/v1/kb-drafts/{draft_id}/chunks?page=<number>


These contain the final, cleaned, ready-to-use content.

ğŸ“Œ I think that the step that gives you full, cleaned content is missing:
POST /api/v1/kb-drafts/{draft_id}/process


This is the step that removes:
unwanted materials
links
menus
headers
UI elements
sidebar navigation
emojies
And gives you true documentation text, split into clean chunks.